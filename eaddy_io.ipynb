{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re \n",
    "\n",
    "import pickle\n",
    "from textblob import TextBlob\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from textblob import Word\n",
    "\n",
    "#from geopy.geocoders import Nominatim \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get your Factiva pdfs into a machine readable format (.txt) move them into this folder and run the cell under the heading 'Reading In PDF Data'. To get individual files as dataframes, run all cells and then pass the file you want to the factiva_parser function:\n",
    "\n",
    "my_dataframe = factiva_parser('My_File.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very Quick Python Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Jupyter Notebook, information is contained in vertically stacked 'cells' that can be used to either display text (like this cell) by selecting the Markdown option from the dropdown menu beneath the tabs at the top of the window or perform Python operations by selecting the Code option from the same drop down. A third option designated 'Raw' also allows for the display of text, but without Markdown formatting. Cells can be edited by double clicking within the cells. Cells are executed either by clicking the arrow to the left of the cell dropdown menu used earlier or by using the shortcut 'Ctrl + Enter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell is rendering code so anything we do not want to be evaluated must be prefaced by a '#' which is referred to as commenting out code.\n",
    "\n",
    "# Uncommented code is evaluated by the Python interpreter as seen below\n",
    "2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Data Types and Basic Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, the python interpreter can take input, perform operations on that input, and return output. There are few limits to the types of operations that can be performed on input, but there are a finite number of ways that input can be represented in Python so we will go over a couple of them here. The most common ways data are represented are int, float, str, and datetime. Any type of data can be stored in a variable which is a name that you give to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an int:  1\n",
      "This is a float:  1.01\n",
      "This is a string:  Hello\n",
      "This is a datetime:  2021-03-25 07:18:19.154571\n"
     ]
    }
   ],
   "source": [
    "print('This is an int: ', 1)\n",
    "print('This is a float: ', 1.01)\n",
    "print('This is a string: ', 'Hello')\n",
    "print('This is a datetime: ', dt.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can assign our data to variable names to call upon them later using the '=' operator.\n",
    "\n",
    "my_integer = 1\n",
    "my_float = 1.01\n",
    "my_string = 'Hello'\n",
    "a_moment_in_time = dt.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when the above cell is run, nothing was printed below the cell. This is because we never performed any action in the cell that would return output. We were only assigning names to certain data values. We can reference those names now and the Python interpreter will return the data that the name refers to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 3, 25, 7, 18, 19, 325286)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_moment_in_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists, Tuples, and Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data can be arranged into several different types of structures in Python. The most basic is called a list and is created using brackets. This is useful for organizing data that you may want to treat in a unified way in the future. Lists are indexed and items can be added to or removed from the list after its creation (the list is said to be mutable). Tuples are very similar to lists except that they are not mutable and are instantiated using parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1.01, 'Hello', datetime.datetime(2021, 3, 25, 7, 18, 19, 325286)]\n",
      "(1, 1.01, 'Hello', datetime.datetime(2021, 3, 25, 7, 18, 19, 325286))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [my_integer, my_float, my_string, a_moment_in_time]\n",
    "print(my_list)\n",
    "\n",
    "my_tuple = (my_integer, my_float, my_string, a_moment_in_time)\n",
    "print(my_tuple)\n",
    "\n",
    "my_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries are Pythons implementation of hash tables. Hash tables are data structures that relate named keys to certain values, known as a key-value pair. This is fundamentally different from lists or tuples in that the indexes are no longer positional, but rather indexed by the key. This key can be thought of as similar to a variable name that references the value inside the dictionary. Dictionaries are instantiated using curly brackets with keys and values being related using semicolons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "for_either_key_or_value\n"
     ]
    }
   ],
   "source": [
    "my_dictionary = {'first_key':1, 'second_key':2, 'you_can_put_whatever_you_want':'for_either_key_or_value'}\n",
    "\n",
    "print(my_dictionary['first_key'])\n",
    "print(my_dictionary['you_can_put_whatever_you_want'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that .json files can be represented in Python as nested dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas and Numpy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way to handle larger amounts of data inside python is in a pandas dataframe. This can be thought of as pythons version of an Excel table or spreadsheet. It is comprised of columns and rows with the convention being that each column is a variable and each row is an observation. There are many different ways to create dataframes, but the most common is to read in from a csv file using pd.read_csv(). Pandas dataframes are very robust ways to manipulate data but have some important differences from simpler data structures. \n",
    "\n",
    "* Columns may contain different data types as in lists or tuples, but this can cause problems when attempting to perform full column calculations\n",
    "* Pandas dataframes can be indexed by either name or location using the .loc and .iloc methods respectively\n",
    "* Pandas contains many ways to manipulate the data inside dataframes, which creates overhead. Pandas dataframes will be slightly slower than simpler representations of data, such as lists, or arrays\n",
    "\n",
    "Numpy arrays are vectorized matrices that allow for increased computational efficiency compared to pandas dataframes. They are MUCH less flexible than pandas dataframes in terms of what operations they allow and are mostly used for more complex mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column1</th>\n",
       "      <th>column2</th>\n",
       "      <th>descriptive_col_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>two_types</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>of_data</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>strings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   column1    column2 descriptive_col_name\n",
       "0        1  two_types                  all\n",
       "1        2    of_data                  the\n",
       "2        3          1              strings"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df = pd.DataFrame({'column1':[1, 2, 3],\n",
    "                      'column2':['two_types', 'of_data', 1],\n",
    "                      'descriptive_col_name':['all', 'the', 'strings']})\n",
    "\n",
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic and Control Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a number of different operations that allow you to add more complex behavior to your python code. Two of these ways are conditionals and control flow statements. Conditionals evaluate some True-False statement and execute different segments of code depending on the result. Control flow statements allow you rerun a block of code a set number of times. The two most common implementations of conditionals and control flow are 'if-else' statements and for loops resepctively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bring it on in\n",
      "It stays outside\n",
      "Bring it on in\n"
     ]
    }
   ],
   "source": [
    "x = {'cat':'small', 'elephant':'large','dog':'in-between'}\n",
    "\n",
    "for i in x:\n",
    "    if x[i]=='large':\n",
    "        print('It stays outside')\n",
    "    else:\n",
    "        print('Bring it on in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that the if statement evaluated whether the value associated with key contained the string 'large'. If it evaluated as True, the first output was returned; otherwise the second value was returned. The for loop allowed this evaluation to be performed for every key value in the dictionary x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in PDF Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell reads all available .pdf files in the local directory and converts the well formatted files into .txt files which are more amenable to processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for root, dirs, files in os.walk(\".\"):\n",
    "#    for filename in files:\n",
    "#        if filename.endswith('.pdf'):\n",
    "#            !pdftotext {filename}\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Factiva Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a good idea to look at the raw text data to make sure it looks similar to the pdf file. Improperly formatted pdf files can be difficult or impossible to convert to machine readable format. Since we are primarily pulling from a single source this shouldn't be a problem but it is important to be aware of the potential issue if using pdf's from other sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Marketing has looked at the succesful PR re-launch of the Johnson and Johnson drug Tylenol. .................................5\\n',\n",
       " \"Johnson and Johnson's Tylenol analgesic capsules have staged a dramatic recovery since seven people in Chicago\\n\",\n",
       " 'died after taking Tylenol... ...............................................................................................................................................6\\n',\n",
       " 'Johnson and Johnson has reported net earnings of $150.3m, or 79 cents a share, for the third quarter of 1983. ..........\\n',\n",
       " '7\\n',\n",
       " 'No Headline ....................................................................................................................................................................8\\n',\n",
       " 'Companies and Markets: Drug problems hit Johnson &Johnson ...................................................................................9\\n',\n",
       " 'JOHNSON & JOHNSON 3RD QTR NET 79C A SHARE VS. OPER. NET 78C ..........................................................10\\n',\n",
       " 'Defense Calls Publicity Goal Of Tylenol Extortion Letters ...........................................................................................11\\n',\n",
       " 'HEADLINERS ..............................................................................................................................................................12\\n',\n",
       " \"'HUMANIZING' CORPORATIONS ...............................................................................................................................13\\n\",\n",
       " 'Tylenol Letter Writer Denies Profit Motive .....................................................................................................................15\\n',\n",
       " 'AROUND THE NATION; Arguments Open in Case Of Tylenol Extortion ....................................................................16\\n',\n",
       " 'AROUND THE NATION; Trial Begins for Suspect In Tylenol Extortion .......................................................................17\\n',\n",
       " \"AROUND THE NATION; Co-Tylenol Ruled Out As Cause of Man's Death .................................................................18\\n\",\n",
       " 'Boots pain-relieving drug given approval in US ............................................................................................................19\\n',\n",
       " 'ADVERTISING; Bristol-Myers Plans Big Coupon Campaign .......................................................................................21\\n',\n",
       " 'JOHNSON & JOHNSON UNIT HAVING TROUBLE WITH 24-TABLET BOTTLES ....................................................22\\n',\n",
       " 'ANNUAL REPORTS: MORE CANDOR .......................................................................................................................23\\n',\n",
       " 'STERLING DRUG CHALLENGES TYLENOL WITH PANADOL BRAND ...................................................................25\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f =  open('./JnJ80_84.txt', 'r')  \n",
    "factiva_raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "factiva_raw[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have successfully read in the pdf file into a text file. Notice that when using the readlines method, the output is a list with each line getting it's own entry in the list. The newline character '\\n' is also left uninterpreted and included in the output. Both of these observations will be important to recognize during processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the data from Factiva in a text file but before we can start to extract information from it we must first convert it to a more organized structure. We will do this by parsing the text using regular expressions. Regular expressions are patterns that allow the user to specify patterns of strings that the regular expression will match. This will allow us to identify useful parts of the text and arrange them accordingly. Regular expressions can be hard interpret due to their concise notation, but a good intro to them and their use can be found at https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by building functions that use regular expressions to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(factiva_by_lines):\n",
    "    \n",
    "    \n",
    "    # The re expressions used here were created by identifying common patterns present in \n",
    "    # the Factiva formatting.\n",
    "    title_end = re.compile('\\.+\\d{1,4}\\\\n')\n",
    "    new_line = re.compile('\\D\\\\n')\n",
    "    article_start = re.compile('\\d{1,3} words$')\n",
    "    \n",
    "    # \n",
    "    current = ''\n",
    "    titles = []\n",
    "    \n",
    "    for line in factiva_by_lines:\n",
    "        \n",
    "        regex_end = title_end.search(line)\n",
    "        regex_new = new_line.search(line)\n",
    "        regex_break = article_start.search(line)\n",
    "        \n",
    "        # Detects if the title portion has ended and breaks the loop if so\n",
    "        if regex_break:\n",
    "            break\n",
    "            \n",
    "        # Handles titles that don't fit on one line\n",
    "        if regex_new:\n",
    "            current += line[:regex_new.span()[0]+1]\n",
    "            continue\n",
    "        \n",
    "        if regex_end:\n",
    "            \n",
    "            # Strips all the dots from the title using the indices in the regular expression\n",
    "            title = line[:regex_end.span()[0]]\n",
    "            current += title\n",
    "            titles.append(current)\n",
    "            current = ''\n",
    "    \n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(factiva_by_lines):\n",
    "    lines = []\n",
    "    article = ''\n",
    "    articles = []\n",
    "    \n",
    "    article_flag = 0\n",
    "    \n",
    "    regex_date = re.compile('^[0-9]?[0-9] (January|February|March|April|May|June|July|August|September|October|November|December) [0-9][0-9][0-9][0-9]$')\n",
    "    regex_end = re.compile('^Document \\S{25}')\n",
    "    \n",
    "    for i in factiva_by_lines:\n",
    "        article_start = regex_date.search(i)\n",
    "        article_end = regex_end.search(i)\n",
    "        \n",
    "        if article_start:\n",
    "            article_flag = 1\n",
    "        if article_end:\n",
    "            articles.append(article)\n",
    "            article = ''\n",
    "            article_flag = 0\n",
    "        if article_flag:\n",
    "            article += i\n",
    "    \n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factiva_title(folder='Eaddy_Factiva'):\n",
    "    for _, _, files in os.walk('Eaddy_Factiva'):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.pdf'):\n",
    "                print(filename)\n",
    "        break\n",
    "\n",
    "def factiva_parser(factiva_txt, stopwords=stop):\n",
    "    f =  open(factiva_txt, 'r')  \n",
    "    factiva_raw = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    titles = get_titles(factiva_raw)\n",
    "    articles = get_articles(factiva_raw)\n",
    "    \n",
    "    df = pd.DataFrame({'titles':titles, 'articles':articles})\n",
    "    \n",
    "    # Preprocessing\n",
    "    # lowercase\n",
    "    df['articles'] = df['articles'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    # symbols\n",
    "    df['articles'] = df['articles'].str.replace('[^\\w\\s]','', regex=True) \n",
    "    # stopwords\n",
    "    df['articles'] = df['articles'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords))\n",
    "    # common Words\n",
    "    freq = pd.Series(' '.join(df['articles']).split()).value_counts()[:10]\n",
    "    print(freq)\n",
    "    #df['articles'] = df['articles'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    # lemmatization\n",
    "    df['articles'] = df['articles'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Johnson and Johnson's Tylenol analgesic capsules have staged a dramatic recovery since seven people in Chicago\n",
      "Johnson and Johnson has reported net earnings of $150.3m, or 79 cents a share, for the third quarter of 1983. ..........\n",
      "Page 1 of 140 © 2020 Factiva, Inc. All rights reserved.\n",
      "Page 1 of 140 © 2020 Factiva, Inc. All rights reserved.\f",
      "In an attempt to recover lost profits due to the poisoned Tylenol scare, Johnson and Johnson has made liability\n",
      "Campaign has pointed out that the \"comeback assignment\" for Tylenol in the USA is being handled by Compton, part\n",
      "McNeil Laboratories, a subsidiary of Johnson and Johnson, has filed suit against a number of insurance companies\n",
      "Information Resources, a part of Hewlett Packard, reports a come back in sales of Johnson and Johnson's product\n",
      "Companies and Markets: Tightening up packaging - TYLENOL TRAGEDY HOLDS WARNING FOR EUROPE............\n",
      "Page 2 of 140 © 2020 Factiva, Inc. All rights reserved.\n",
      "Page 3 of 140 © 2020 Factiva, Inc. All rights reserved.\n",
      "Page 4 of 140 © 2020 Factiva, Inc. All rights reserved.\n",
      "Page 4 of 140 © 2020 Factiva, Inc. All rights reserved.\f",
      "Marketing has looked at the succesful PR re-launch of the Johnson and Johnson drug Tylenol.\n",
      "Page 4 of 140 © 2020 Factiva, Inc. All rights reserved.\f",
      "Marketing has looked at the succesful PR re-launch of the Johnson and Johnson drug Tylenol.Marketing has looked at the succesful PR re-launch of the Johnson and Johnson drug Tylenol.\n",
      "tylenol     471\n",
      "johnson     408\n",
      "said        331\n",
      "company     272\n",
      "new         224\n",
      "million     195\n",
      "capsules    189\n",
      "1982        145\n",
      "sales       142\n",
      "market      129\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "factiva_df = factiva_parser('./JnJ80_84.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "factiva_df['articles'] = factiva_df['articles'].apply(lambda x: x.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Scaling Factor')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Corpus Scope vs Article Scope\n",
    "def fy(x, floor=0.2, shape=.6):\n",
    "    return ((1/np.exp(x*shape))*(1-floor) + floor)\n",
    "\n",
    "x = list(range(50))\n",
    "y = [fy(i) for i in x]\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Years Ago')\n",
    "plt.ylabel('Scaling Factor')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NRC Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emotion</th>\n",
       "      <th>association</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aback</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aback</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aback</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aback</td>\n",
       "      <td>fear</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aback</td>\n",
       "      <td>joy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aback</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aback</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aback</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aback</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aback</td>\n",
       "      <td>trust</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abacus</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>abacus</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>abacus</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>abacus</td>\n",
       "      <td>fear</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>abacus</td>\n",
       "      <td>joy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>abacus</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>abacus</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>abacus</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>abacus</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>abacus</td>\n",
       "      <td>trust</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word       emotion  association\n",
       "0    aback         anger            0\n",
       "1    aback  anticipation            0\n",
       "2    aback       disgust            0\n",
       "3    aback          fear            0\n",
       "4    aback           joy            0\n",
       "5    aback      negative            0\n",
       "6    aback      positive            0\n",
       "7    aback       sadness            0\n",
       "8    aback      surprise            0\n",
       "9    aback         trust            0\n",
       "10  abacus         anger            0\n",
       "11  abacus  anticipation            0\n",
       "12  abacus       disgust            0\n",
       "13  abacus          fear            0\n",
       "14  abacus           joy            0\n",
       "15  abacus      negative            0\n",
       "16  abacus      positive            0\n",
       "17  abacus       sadness            0\n",
       "18  abacus      surprise            0\n",
       "19  abacus         trust            1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"./Eaddy_Factiva/NRC-Suite-of-Sentiment-Emotion-Lexicons/NRC-Suite-of-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "emolex_df = pd.read_csv(filepath,  names=[\"word\", \"emotion\", \"association\"], sep='\\t')\n",
    "emolex_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salience Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_decay_fx(x, floor=0.5, shape=0.5):\n",
    "    return ((1/np.exp(x*shape))*(1-floor) + floor)\n",
    "\n",
    "def get_valence(article):\n",
    "    sentiment, subjectivity = TextBlob(article).sentiment\n",
    "    return sentiment\n",
    "\n",
    "def get_factiva_date(article):\n",
    "    regex_date = re.compile('^[0-9]?[0-9] (january|february|march|april|may|june|july|august|september|october|november|december) [0-9][0-9][0-9][0-9]')\n",
    "    date = regex_date.search(article)\n",
    "    date = dt.strptime(date[0], '%d %B %Y')\n",
    "    return date\n",
    "\n",
    "def get_factiva_proximity(article, time_decay_params):\n",
    "    \n",
    "    now = dt.now()\n",
    "    date = get_factiva_date(article)\n",
    "    yrs_ago = (now-date).days/365\n",
    "    time_decay = time_decay_fx(yrs_ago, time_decay_params)\n",
    "    \n",
    "    # In the future maybe use machine learning to contextually identify locations. \n",
    "    return time_decay\n",
    "    \n",
    "\n",
    "\n",
    "def get_crisis_emotions(article,nrc_df=emolex_df):\n",
    "    # Need to associate negative and positive with the words they correspond to and modify meaning accordingly\n",
    "    article_dict = {'anger':0,\n",
    "                    'negative':0,\n",
    "                    'positive':0,\n",
    "                    'anticipation':0,\n",
    "                    'disgust':0,\n",
    "                    'fear':0,\n",
    "                    'joy':0,\n",
    "                    'sadness':0,\n",
    "                    'surprise':0,\n",
    "                    'trust':0}\n",
    "    nrc_words = pd.unique(nrc_df['word'])\n",
    "    article_words =nltk.word_tokenize(article)\n",
    "    \n",
    "    # instead of iterating, make it vectorized for improved performance\n",
    "    for i in article_words:\n",
    "        if i in nrc_words:\n",
    "            subset = nrc_df[nrc_df['word']==i]\n",
    "            for j in range(subset.shape[0]):\n",
    "                idx = min(subset.index)\n",
    "                key = subset.loc[idx+j,'emotion']\n",
    "                val = subset.loc[idx+j, 'association']\n",
    "                article_dict[key] += val\n",
    "    article_dict['total'] = len(article_words)\n",
    "    return article_dict\n",
    "            \n",
    "    \n",
    "    \n",
    "# get_crisis_type will difficult as that is obfuscated by fluid subject object relationships when identifying victimhood.\n",
    "#def get_crisis_type(article):\n",
    "    #return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 3,\n",
       " 'negative': 8,\n",
       " 'positive': 7,\n",
       " 'anticipation': 4,\n",
       " 'disgust': 4,\n",
       " 'fear': 7,\n",
       " 'joy': 3,\n",
       " 'sadness': 4,\n",
       " 'surprise': 2,\n",
       " 'trust': 4,\n",
       " 'total': 81}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_crisis_emotions(factiva_df.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "factiva_df['Date'] = factiva_df['articles'].apply(get_factiva_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.33698630136986\n",
      "1983-12-02 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1983, 12, 2, 0, 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((dt.now()- get_factiva_date(factiva_df.iloc[0,1])).days/365)\n",
    "print(get_factiva_date(factiva_df.iloc[0,1]))\n",
    "get_factiva_date(factiva_df.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>articles</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing has looked at the succesful PR re-la...</td>\n",
       "      <td>2 december 1983 marketing mktg 12 english c 19...</td>\n",
       "      <td>1983-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnson and Johnson's Tylenol analgesic capsul...</td>\n",
       "      <td>25 november 1983 campaign cmpn 8 english c 198...</td>\n",
       "      <td>1983-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Johnson and Johnson has reported net earnings ...</td>\n",
       "      <td>28 october 1983 financial time ftft 25 english...</td>\n",
       "      <td>1983-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Companies and Markets: Drug problems hit Johns...</td>\n",
       "      <td>28 october 1983 new york time nytf late city f...</td>\n",
       "      <td>1983-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOHNSON &amp; JOHNSON 3RD QTR NET 79C A SHARE VS. ...</td>\n",
       "      <td>28 october 1983 financial time ftft page 17 en...</td>\n",
       "      <td>1983-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>TYLENOL COUPONS TO BE MAILED NATIONALLY BEGINN...</td>\n",
       "      <td>18 october 1982 wall street journal j english ...</td>\n",
       "      <td>1982-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Drug Makers Ask U.S. Rules On Packaging</td>\n",
       "      <td>15 october 1982 washington post wp english cop...</td>\n",
       "      <td>1982-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Suspect in Tylenol Case Once Held in '78 Death</td>\n",
       "      <td>15 october 1982 new york time nytf late city f...</td>\n",
       "      <td>1982-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>LEGAL BATTLE OVER WHO IS LIABLE FOR THE TYLENO...</td>\n",
       "      <td>14 october 1982 new york time nytf late city f...</td>\n",
       "      <td>1982-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>F.B.I. SEEKS MAN OF TYLENOL EXTORTION CHARGE</td>\n",
       "      <td>14 october 1982 new york time nytf late city f...</td>\n",
       "      <td>1982-10-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               titles  \\\n",
       "0   Marketing has looked at the succesful PR re-la...   \n",
       "1   Johnson and Johnson's Tylenol analgesic capsul...   \n",
       "2   Johnson and Johnson has reported net earnings ...   \n",
       "3   Companies and Markets: Drug problems hit Johns...   \n",
       "4   JOHNSON & JOHNSON 3RD QTR NET 79C A SHARE VS. ...   \n",
       "..                                                ...   \n",
       "93  TYLENOL COUPONS TO BE MAILED NATIONALLY BEGINN...   \n",
       "94           Drug Makers Ask U.S. Rules On Packaging    \n",
       "95    Suspect in Tylenol Case Once Held in '78 Death    \n",
       "96  LEGAL BATTLE OVER WHO IS LIABLE FOR THE TYLENO...   \n",
       "97      F.B.I. SEEKS MAN OF TYLENOL EXTORTION CHARGE    \n",
       "\n",
       "                                             articles       Date  \n",
       "0   2 december 1983 marketing mktg 12 english c 19... 1983-12-02  \n",
       "1   25 november 1983 campaign cmpn 8 english c 198... 1983-11-25  \n",
       "2   28 october 1983 financial time ftft 25 english... 1983-10-28  \n",
       "3   28 october 1983 new york time nytf late city f... 1983-10-28  \n",
       "4   28 october 1983 financial time ftft page 17 en... 1983-10-28  \n",
       "..                                                ...        ...  \n",
       "93  18 october 1982 wall street journal j english ... 1982-10-18  \n",
       "94  15 october 1982 washington post wp english cop... 1982-10-15  \n",
       "95  15 october 1982 new york time nytf late city f... 1982-10-15  \n",
       "96  14 october 1982 new york time nytf late city f... 1982-10-14  \n",
       "97  14 october 1982 new york time nytf late city f... 1982-10-14  \n",
       "\n",
       "[98 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factiva_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([43., 17.,  9.,  9.,  4.,  1.,  0.,  2.,  5.,  8.]),\n",
       " array([723832. , 723873.4, 723914.8, 723956.2, 723997.6, 724039. ,\n",
       "        724080.4, 724121.8, 724163.2, 724204.6, 724246. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(factiva_df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11082251082251082\n",
      "0.16666666666666666\n",
      "0.05481748625074361\n",
      "~~~~~~\n",
      "1\n",
      "9\n",
      "[1. 4. 7. 3. 9. 6. 5. 3. 5. 5. 7. 5. 4. 6. 8. 6. 9. 1. 4. 5. 6. 1. 2. 6.\n",
      " 7. 3. 4. 9. 5. 7. 3. 6. 8. 3. 5. 8. 8. 4. 3. 5. 7. 7. 5. 5. 2. 8. 2. 2.\n",
      " 6. 4. 3. 6. 1. 8. 9. 2. 6. 3. 6. 2. 3. 7. 6. 1. 5. 9. 1. 2. 3. 8. 6. 1.\n",
      " 9. 8. 3. 1. 6. 6. 3. 2. 1. 8. 4. 6. 7. 1. 4. 2. 7. 7. 3. 8. 3. 5. 6. 4.\n",
      " 7. 7.]\n",
      "0.013759860049622362\n",
      "0.016042670657491303\n",
      "-12.407633450490895\n"
     ]
    }
   ],
   "source": [
    "# Hanks Ranking of every article in the 100 as a test\n",
    "# done w.r.t. tone and relevance ie a negative article \n",
    "# about J&J's competitor is good\n",
    "\n",
    "# 4 and 53 are having problems parsing. Fix the get_titles() function\n",
    "\n",
    "def t_score(dat0, dat1):\n",
    "    t = (np.mean(dat0)-np.mean(dat1))/(np.std(dat1)/np.sqrt(len(dat1)))\n",
    "    return t\n",
    "    \n",
    "\n",
    "hss = [6, 8, 5, #3,\n",
    "        2, 3, 4, 3, 6, 3,\n",
    "      2, 2, 4, 7, 6, 4, 6, 5, 3, 4,\n",
    "      6, 7, 2, 3, 2, 6, 2, 5, 4, 4,\n",
    "      5, 5, 6, 6, 8, 6, 6, 5, 4, 4,\n",
    "      6, 6, 7, 8, 7, 9, 8, 7, 4, 3,\n",
    "      5, 4, 4, #2,\n",
    "        6, 7, 7, 8, 2, 7,\n",
    "      3, 6, 1, 4, 4, 6, 6, 6, 7, 6,\n",
    "      4, 3, 4, 2, 6, 3, 3, 3, 6, 2,\n",
    "      3, 2, 2, 1, 3, 2, 2, 1, 7, 7,\n",
    "      1, 8, 5, 6, 7, 6, 5, 7, 3, 6]\n",
    "\n",
    "hss_scaled = minmax_scale(hss, (-0.17,0.17))\n",
    "\n",
    "factiva_df['sent_valence'] = factiva_df['articles'].apply(get_valence, 1)\n",
    "factiva_df.head()\n",
    "print(min(factiva_df['sent_valence']))\n",
    "print(max(factiva_df['sent_valence']))\n",
    "print(np.mean(factiva_df['sent_valence']))\n",
    "\n",
    "sent_val = factiva_df['sent_valence']\n",
    "print('~~~~~~')\n",
    "\n",
    "print(min(hss))\n",
    "print(max(hss))\n",
    "\n",
    "random = np.round(minmax_scale(np.random.random(98), (.1, .9)), 1)*10\n",
    "random_scaled = minmax_scale(random, (-.17,.17))\n",
    "print(random)\n",
    "print(metrics.mean_squared_error(hss_scaled, sent_val))\n",
    "print(metrics.mean_squared_error(random_scaled, sent_val))\n",
    "\n",
    "print(t_score(hss_scaled, sent_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to pulling data from Factiva or other traditional print media, we may also use data from social media platforms such as Twitter in our analysis.  .In order to use the Twitter API we will need to create a Twitter developer account and get assigned an API key as well as a bearer token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def auth():\n",
    "    return os.environ.get(\"BEARER_TOKEN\")\n",
    "\n",
    "\n",
    "# Might wanna pass this a dict\n",
    "def create_url(user_id = 20457806,\n",
    "               start_time='2009-02-19T19:12:13.000Z', \n",
    "               end_time='2018-03-06T00:00:00Z'):\n",
    "    # Replace with user ID below\n",
    "    # Start Time limited to 2010-11-06T00:00:00-00:00\n",
    "    #return \"https://api.twitter.com/2/users/{}/tweets?end_time={}\".format(user_id, end_time)\n",
    "    return \"https://api.twitter.com/2/users/{}/tweets?end_time={}\".format(user_id, end_time)\n",
    "\n",
    "def user_created_at(user_id = 20457806):\n",
    "    return 'https://api.twitter.com/2/users/{}'.format(user_id)\n",
    "\n",
    "def get_params(page=''):\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "    page = page\n",
    "    if page:\n",
    "         return {\"tweet.fields\": \"created_at\", \"pagination_token\":page}\n",
    "    else:\n",
    "        return {\"tweet.fields\": \"created_at\", }\n",
    "\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params):\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def main(page='', state=[]):\n",
    "    bearer_token = auth()\n",
    "    url = create_url()\n",
    "    headers = create_headers(bearer_token)\n",
    "    params = get_params(page)\n",
    "    json_response = connect_to_endpoint(url, headers, params)\n",
    "    state=state+json_response['data']\n",
    "    try:\n",
    "        return main(page=json_response['meta']['next_token'], state=state)\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        return state\n",
    "    #return json.dumps(json_response, indent=4, sort_keys=True)\n",
    "\n",
    "def get_acct_age(user_id=20457806):\n",
    "    bearer_token=auth()\n",
    "    url='https://api.twitter.com/2/users/{}'.format(user_id)\n",
    "    headers = create_headers(bearer_token)\n",
    "    params={'user.fields':'created_at'}\n",
    "    json_response = connect_to_endpoint(url, headers, params)\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "'next_token'\n"
     ]
    }
   ],
   "source": [
    "x = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3236 most recent tweets\n",
    "recent_tweets = open('recent_tweets.pkl', 'ab') \n",
    "      \n",
    "pickle.dump(x, recent_tweets)                      \n",
    "recent_tweets.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = []\n",
    "with (open(\"recent_tweets.pkl\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            objects.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3236"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(objects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_at = [i['created_at'] for i in objects[0]]\n",
    "tweet_id = [j['id'] for j in objects[0]]\n",
    "text = [k['text'] for k in objects[0]]\n",
    "\n",
    "df = pd.DataFrame({'created_at':created_at,\n",
    "                 'tweet_id':tweet_id,\n",
    "                 'text':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-02-21T19:00:06.000Z'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4581 unique tokens. Distilled to 4581 top words.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "NUM_TOP_WORDS = None \n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(factiva_df.articles)\n",
    "# save as sequences with integers replacing words\n",
    "sequences = tokenizer.texts_to_sequences(factiva_df.articles)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 516783 word vectors.\n",
      "Embedding Shape: (4582, 300) \n",
      " Total words found: 3842 \n",
      " Percentage: 83.84984722828459\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 300\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open(\"/home/hank/Downloads/coding_dls/numberbatch/numberbatch-en.txt\", encoding=\"utf-8\")\n",
    "\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "found_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found_words = found_words+1\n",
    "    else:\n",
    "        print(word)\n",
    "        \n",
    "\n",
    "print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
    "      \"Total words found:\",found_words, \"\\n\",\n",
    "      \"Percentage:\",100*found_words/embedding_matrix.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
